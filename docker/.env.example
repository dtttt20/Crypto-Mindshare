## TimescaleDB database for storing mindshare data
TIMESCALEDB_USER=postgres
TIMESCALEDB_PASSWORD=password
TIMESCALEDB_DB=postgres

MINDSHARE_DB_URL=postgres://postgres:password@timescaledb:5432/mindshare
MINDSHARE_BASE_DB_URL=postgres://postgres:password@timescaledb:5432
MINDSHARE_DEFAULT_DB_URL=postgres://postgres:password@timescaledb:5432/postgres

## Redis database for caching mindshare data
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0

##________________________________USER CONFIG________________________________##
## Database to pull tweets from
# TWEET_DATABASE_HOST=db_host
# TWEET_DATABASE_NAME=db_name
# TWEET_DATABASE_USERNAME=db_username
# TWEET_DATABASE_PASSWORD=db_password
# TWEET_DATABASE_PORT=5432

## Sample tweet database for testing
TWEET_DATABASE_HOST=tweet_archive_db
TWEET_DATABASE_NAME=tweet_archive
TWEET_DATABASE_USERNAME=postgres
TWEET_DATABASE_PASSWORD=postgres
TWEET_DATABASE_PORT=5432

## User LLM API keys
OPENAI_API_KEY=openai_api_key
PERPLEXITY_API_KEY=perplexity_api_key
OPENROUTER_API_KEY=openrouter_api_key

## Starting timestamp for processing tweets
## This is the timestamp of the first tweet to process, in UTC timezone
## This is used to resume processing from the last processed tweet
## If you want to process past tweets, set this to the first tweet in the database, the default is 2025-02-24T11:00:00Z
## The sample tweet database has tweets from 2025-03-03 20:41:14Z to 2025-03-04 21:19:27Z

## WARNING: If you set this to an early timestamp, it will use a lot of tokens
## and be very expensive due to the checking of the project and token existence
STARTING_TIMESTAMP=2025-03-03T00:00:00Z

## Maximum number of tweets to process per batch (each time the mindshare processor is run, default is 100)
MAX_TWEETS_PER_BATCH=50

## Maximum number of concurrent tweets to process
MAX_CONCURRENT_TWEETS=200

## Maximum number of concurrent validations to perform 
## (limited by Openrouter or Perplexity rate limits)
MAX_CONCURRENT_VALIDATIONS=70

## Maximum number of concurrent OpenAI API calls allowed
## (based on your OpenAI api key rate limits, default rate limit is 4900)
OPENAI_MAX_CONCURRENT_REQUESTS=4900
TIME_WINDOW_SECONDS=60

## Maximum number of concurrent Openrouter API calls allowed
## (based on your Openrouter api key rate limits, default is 65 based on having minimum 10 credits loaded)
OPENROUTER_MAX_CONCURRENT_REQUESTS=65
TIME_WINDOW_SECONDS=10

## Maximum number of concurrent perplexity api calls to perform 
## (based on your perplexity api key rate limits, default is 60)
PERPLEXITY_MAX_CONCURRENT_REQUESTS=60
TIME_WINDOW_SECONDS=60